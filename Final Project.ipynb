{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "CS696 Big Data\n",
    "\n",
    "Professor Whitney\n",
    "\n",
    "Team:\n",
    "\n",
    "Kristi Werry - 823386935\n",
    "\n",
    "William Ritchie - 815829203"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import to_timestamp, max, length\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.functions import lit, avg, format_number, split, size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Datasets(sqlContext, files, schema):\n",
    "    data = sqlContext.createDataFrame([], schema=schema)\n",
    "    for file in files:\n",
    "        temp_data = sqlContext.read.csv(path=file[0], schema=schema, dateFormat=\"yy.dd.MM\", timestampFormat=\"yyyy-MM-dd\")\n",
    "        temp_data = temp_data.withColumn(\"country\", lit(file[1]))\n",
    "        data = data.union(temp_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SparkSession.builder.appName(\"FinalProjectYoutube\").getOrCreate();\n",
    "root_dir = \"youtube-new/\";\n",
    "\n",
    "# Set up the schema for reading in the data sets into a dataframe\n",
    "customSchema = StructType([\n",
    "  StructField(\"video_id\", StringType(), True),\n",
    "  StructField(\"trending_date\", DateType(), True),\n",
    "  StructField(\"title\", StringType(), True),\n",
    "  StructField(\"channel_title\", StringType(), True),\n",
    "  StructField(\"category_id\", StringType(), True),\n",
    "  StructField(\"publish_time\", TimestampType(), True),\n",
    "  StructField(\"tags\", StringType(), True),\n",
    "  StructField(\"views\", IntegerType(), True),\n",
    "  StructField(\"likes\", IntegerType(), True),\n",
    "  StructField(\"dislikes\", IntegerType(), True),\n",
    "  StructField(\"comment_count\", IntegerType(), True),\n",
    "  StructField(\"thumbnail_link\", StringType(), True),\n",
    "  StructField(\"comments_disabled\", BooleanType(), True),\n",
    "  StructField(\"ratings_disabled\", BooleanType(), True),\n",
    "  StructField(\"video_error_or_removed\", BooleanType(), True),\n",
    "  StructField(\"description\", StringType(), True),\n",
    "  StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Associate csv files with respective countries\n",
    "data_files = [\n",
    "   (root_dir + \"CAvideos.csv\", \"Canada\"),\n",
    "   (root_dir + \"DEvideos.csv\", \"Germany\"),\n",
    "   (root_dir + \"FRvideos.csv\", \"France\"),\n",
    "   (root_dir + \"GBvideos.csv\", \"England\"),\n",
    "   (root_dir + \"INvideos.csv\", \"India\"),\n",
    "   (root_dir + \"JPvideos.csv\", \"Japan\"),\n",
    "   (root_dir + \"KRvideos.csv\", \"South Korea\"),\n",
    "   (root_dir + \"MXvideos.csv\", \"Mexixo\"),\n",
    "   (root_dir + \"RUvideos.csv\", \"Russia\"),\n",
    "   (root_dir + \"USvideos.csv\", \"US\"),\n",
    "]\n",
    "\n",
    "us_data_file = [\n",
    "    (root_dir + \"USvideos.csv\", \"US\")\n",
    "]\n",
    "\n",
    "# Read in datasets\n",
    "youtube_data_df = Load_Datasets(sqlContext, data_files, customSchema)\n",
    "us_data_df = Load_Datasets(sqlContext, us_data_file, customSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When dropping the duplicate rows based on video_id we found that half of the dataset gets dropped.  So we now look at the \n",
    "# duplicate rows to find out more information about what is going on.  Looking at the duplicate rows you can see that the \n",
    "# same video can be trending for mulitple days and in different countries causing the same video to exist in multiple rows.\n",
    "# We decided the \"duplicates\" were not actually truely duplicate rows, the information provided by these multiple entries \n",
    "# is still useful.  We determined a truely duplicate row requires the same: \"video_id\", \"views\", \"likes\", \"dislikes\",\n",
    "# \"country\", and \"trending_date\" column values.\n",
    "pandasdf = pd.DataFrame(data=youtube_data_df.take(100000), columns=youtube_data_df.columns)\n",
    "\n",
    "pandas_df = pandasdf.loc[pandasdf['video_id'].duplicated()]\n",
    "\n",
    "# We realized that a number of the duplicates have a \"\\n\" for the video id, we decided to filter those rows out since\n",
    "# they contain no useful information.  We remove these rows later on when dropping NA values from the dataset.\n",
    "pandas_df = pandas_df[pandas_df['video_id'] != \"\\\\n\"]\n",
    "\n",
    "# The video id value was manually selected by viewing the resulting dataframe from the previous line. You can see\n",
    "# that this video was trending for multiple days and in mulitple countries, hence why it has mulitple rows in the \n",
    "# the dataset.\n",
    "pandasdf[pandasdf['video_id'] == \"n1WpP7iowLc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The previous section determined that video_id was not sufficient in determining a truly duplicate row.  The following\n",
    "# are a combination of the columns we think determine a truly duplicate row.  Meaning, if two rows have the same value\n",
    "# in all of the below columns, then those two rows are indeed duplicates.\n",
    "compare_duplicate_cols = [\"video_id\", \"views\", \"likes\", \"dislikes\", \"country\", \"trending_date\"]\n",
    "row_count_with_dup = youtube_data_df.count()\n",
    "\n",
    "# Drop duplicate rows\n",
    "youtube_data_df = youtube_data_df.dropDuplicates(compare_duplicate_cols)\n",
    "\n",
    "# View duplicate row count information\n",
    "num_duplicates = row_count_with_dup - youtube_data_df.count()\n",
    "print(\"Number of duplicates: \" + str(num_duplicates))\n",
    "print(\"Remaining number of rows: \" + str(youtube_data_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle NA's and Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are a list of the columns that we determined should not contain a null or NA value.  We could have\n",
    "# done this when specifying the schema when we were importing the data, but we felt it necessary to learn more information\n",
    "# about columns that contain nulls and NAs.  After playing around with the data we found that many of the values in the\n",
    "# description column were NA.  We decided that this was okay because some videos might not have a description, thus\n",
    "# this is why description is not included in the below list\n",
    "no_nan_cols = [\"video_id\", \"trending_date\", \"title\", \n",
    "        \"channel_title\", \"category_id\",  \n",
    "        \"tags\", \"views\", \"likes\", \"dislikes\", \"comment_count\", \n",
    "        \"thumbnail_link\", \"comments_disabled\", \"ratings_disabled\", \n",
    "        \"video_error_or_removed\", \"country\"]\n",
    "\n",
    "row_count_with_nans = youtube_data_df.count()\n",
    "\n",
    "# Drop nans from these columns\n",
    "youtube_data_df = youtube_data_df.na.drop(subset=no_nan_cols) \n",
    "\n",
    "# View duplicate row count information\n",
    "num_nans = row_count_with_nans - youtube_data_df.count()\n",
    "print(\"Number of null,nans, and na's: \" + str(num_nans))\n",
    "print(\"Remaining number of rows: \" + str(youtube_data_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We decided that it was advantageous to replace all of the nulls in the description column with empty strings, this\n",
    "# way we do not need to check for nulls later on when working with this column in the dataset\n",
    "youtube_data_df = youtube_data_df.fillna(\"\", subset=\"description\")\n",
    "\n",
    "# Check that the description column contains no nulls\n",
    "youtube_data_df.where(youtube_data_df.description.isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping duplicates but keeping max views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicate rows with the same video id and country, and keep the ones with the largest view count\n",
    "nodup_data = youtube_data_df.orderBy(col(\"views\").desc()).dropDuplicates([\"video_id\", \"country\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Averages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a dataframe, a grouping, and a column name and returns \n",
    "# the average of that column based on the group\n",
    "def average_by_group(df, group, column):\n",
    "    return df.groupBy(group).agg(avg(col(column)).alias(\"average\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Out of the trending videos, average number of views\n",
    "#change to not duplicate data instead of all_data\n",
    "average_by_group(nodup_data, \"country\", \"views\").toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_by_group(nodup_data, \"country\", \"likes\").toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_by_group(nodup_data, \"country\", \"dislikes\").toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "average_by_group(nodup_data, \"country\", \"comment_count\").toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing removing duplicates\n",
    "data = [\n",
    "    ('a', 5, 'e'),\n",
    "    ('a', 8, 'd'),\n",
    "    ('a', 7, 'e'),\n",
    "    ('b', 1, 'f'),\n",
    "    ('b', 3, 'g')\n",
    "]\n",
    "df = sqlContext.createDataFrame(data, [\"A\", \"B\", \"C\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy(col(\"B\").desc()).dropDuplicates([\"A\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(['A', 'C']).agg(max('B')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding correlation\n",
    "something wrong with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "col = [\"views\", 'likes', 'dislikes', 'comment_count']\n",
    "inputdf = nodup_data.select(*col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to vector column first\n",
    "vector_col = \"corr_features\"\n",
    "assembler = VectorAssembler(inputCols=inputdf.columns, outputCol=vector_col)\n",
    "df_vector = assembler.transform(inputdf).select(vector_col)\n",
    "\n",
    "# get correlation matrix\n",
    "matrix = Correlation.corr(df_vector, vector_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.collect()[0][\"pearson({})\".format(vector_col)].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finding average title and description length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titlelen = nodup_data.withColumn('titleLength', length('title'))\n",
    "average_by_group(titlelen, 'country', 'titleLength').toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desclen = nodup_data.withColumn('descriptionLength', length('description'))\n",
    "average_by_group(desclen, 'country', 'descriptionLength').toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## possible word cloud\n",
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.pyplot as plt\n",
    "stopwords = set(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
    "##takes a long ass time for descriptions ***********~40 mins*************\n",
    "# iterate through the column\n",
    "def wordCloud(df, column, s):\n",
    "    comment_words = ''\n",
    "    for val in df[column]: \n",
    "      \n",
    "        # typecaste each val to string \n",
    "        val = str(val) \n",
    "  \n",
    "        # split the value \n",
    "        tokens = val.split(s) \n",
    "      \n",
    "        # Converts each token into lowercase \n",
    "        for i in range(len(tokens)): \n",
    "            tokens[i] = tokens[i].lower() \n",
    "      \n",
    "        comment_words += \" \".join(tokens)+\" \"\n",
    "  \n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                    background_color ='black', \n",
    "                    stopwords = stopwords, \n",
    "                    min_font_size = 10).generate(comment_words)\n",
    "\n",
    "    # plot the WordCloud image                        \n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = us_data_df.toPandas()\n",
    "test = test[test['title'].notna()]\n",
    "test = test[test['description'].notna()]\n",
    "wordCloud(test, 'title', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is gunna take a very long time!!!!!!!!!!!!!!!!!!!!!!1\n",
    "#wordCloud(test, 'description', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword frequency count (us data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyWords(df, column, s, amount):\n",
    "    temp = Counter(\" \".join(df[column].str.lower()).split(s)).most_common(amount)\n",
    "    pd.DataFrame(temp, columns=['word', 'count']).plot(kind='bar', x='word', y='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyWords(test, \"title\", \" \", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyWords(test, \"description\", \" \", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyWords_filtered(df, column, amount):\n",
    "    pattern = '|'.join(['\\|', ';', '-', '\\(', '\\)', '\\[', '\\]', '&', ' ', '\\.', '\\,', '\\:', '\\/', '\\'', '\\!', '\\$', '’', '\\\\\\\\', '\\'', '\\\"'])\n",
    "    x = df[column].str.lower().str.replace(pattern, ' ').str.replace(r'\\b(\\w{1,3})\\b', '')\n",
    "    temp = Counter(\" \".join(x).split()).most_common(amount)\n",
    "    pd.DataFrame(temp, columns=['word', 'count']).plot(kind='bar', x='word', y='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyWords_filtered(test, \"title\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyWords_filtered(test, \"description\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most common tags\n",
    "keyWords(test, \"tags\", \"|\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word cloud of tags\n",
    "wordCloud(test, 'tags', '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average number of tags\n",
    "#first split the tags to get an array of tags per video (no duplicates)\n",
    "#then add a new column with the count of the number of tags\n",
    "#then find the average number of tags with the function\n",
    "data_tag_count = nodup_data.withColumn(\"tags\", split(\"tags\", \"\\|\")).withColumn(\"tag_count\", size(\"tags\"))\n",
    "average_by_group(data_tag_count, \"country\", \"tag_count\").toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## total num views by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show total number of views by date\n",
    "views_by_date = youtube_data_df.groupBy(\"trending_date\").sum().orderBy(\"trending_date\").toPandas()\n",
    "pd.DataFrame(views_by_date, columns=['trending_date', 'sum(views)']).plot(kind='line', x='trending_date', y='sum(views)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to see the total number of views of trending videos spike a certain times of the year. We can see the total number increased in Feb of 2018 and decline in the middle of may. I hypothesize that this is because younger people mostly contribute to the status of trending videos and during these times, these people are usually busy with tests  or vacations instead of watching YouTube videos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keywords compare to trending date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = youtube_data_df#.groupBy(\"trending_date\")\n",
    "from pyspark.sql.functions import *\n",
    "#df = sqlContext.createDataFrame([(\"2009-01-03\",[\"1\",\"some\"]),(\"2009-01-09\",[\"14\",\"thing\"]),(\"2009-01-10\",[\"61\",\"here\"])], [\"day\",\"bitcoin_total\"])\n",
    "df = sqlContext.createDataFrame([(\"2009-01-03\",\"some thing\"),(\"2009-01-09\", \"thing here\"),(\"2009-01-10\", \" there here\")], [\"day\",\"bitcoin_total\"])\n",
    "\n",
    "df.show()    \n",
    "\n",
    "#concat(col(\"k\"), lit(\" \"), col(\"v\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.withColumn(\"week_strt_day\",date_sub(next_day(col(\"day\"),\"sunday\"),7)).groupBy(\"week_strt_day\").agg(concat_ws(\", \", collect_list(df.bitcoin_total)).alias(\"bitcoin_total\")).orderBy(\"week_strt_day\")\n",
    "new_df = new_df.toPandas()\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '|'.join(['\\|', ';', '-', '\\(', '\\)', '\\[', '\\]', '&', ' ', '\\.', '\\,', '\\:', '\\/', '\\'', '\\!', '\\$', '’', '\\\\\\\\', '\\'', '\\\"'])\n",
    "new_df[\"bitcoin_total\"] = new_df[\"bitcoin_total\"].str.lower().str.replace(pattern, ' ').str.replace(r'\\b(\\w{1,3})\\b', '')\n",
    "res = new_df[\"bitcoin_total\"].str.split().apply(pd.value_counts)\n",
    "s = pd.DataFrame(res.idxmax(axis=1), columns=[\"word\"])\n",
    "s[\"value\"] = pd.DataFrame(res.max(axis=1), columns=['value'])\n",
    "s[\"week\"] = new_df[\"week_strt_day\"]\n",
    "s.sort_values('week')#.plot(kind='bar', x='word', y='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = us_data_df.withColumn(\"week_strt_day\",date_sub(next_day(col(\"trending_date\"),\"sunday\"),7)).groupBy(\"week_strt_day\").agg(concat_ws(\", \", collect_list(us_data_df.title)).alias(\"title\")).orderBy(\"week_strt_day\")\n",
    "new_df = new_df.toPandas().dropna()\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '|'.join(['\\|', ';', '-', '\\(', '\\)', '\\[', '\\]', '&', ' ', '\\.', '\\,', '\\:', '\\/', '\\'', '\\!', '\\$', '’', '\\\\\\\\', '\\'', '\\\"'])\n",
    "new_df[\"title\"] = new_df[\"title\"].str.lower().str.replace(pattern, ' ').str.replace(r'\\b(\\w{1,3})\\b', '')\n",
    "res = new_df[\"title\"].str.split().apply(pd.value_counts)\n",
    "s = pd.DataFrame(res.idxmax(axis=1), columns=[\"word\"])\n",
    "s[\"value\"] = pd.DataFrame(res.max(axis=1), columns=['value'])\n",
    "s[\"week\"] = new_df[\"week_strt_day\"]\n",
    "s.sort_values('week').plot(kind='bar', x='word', y='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
