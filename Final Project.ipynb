{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "CS696 Big Data\n",
    "\n",
    "Professor Whitney\n",
    "\n",
    "Team:\n",
    "\n",
    "Kristi Werry - 823386935\n",
    "\n",
    "William Ritchie - 815829203"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Datasets(sqlContext, files, schema):\n",
    "    data = sqlContext.createDataFrame([], schema=schema)\n",
    "    for file in files:\n",
    "        temp_data = sqlContext.read.csv(path=file[0], schema=schema, dateFormat=\"yy.dd.MM\", timestampFormat=\"yyyy-MM-dd\")\n",
    "        temp_data = temp_data.withColumn(\"country\", lit(file[1]))\n",
    "        data = data.union(temp_data)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def keyWords(df, column, s, amount):\n",
    "    temp = Counter(\" \".join(df[column].str.lower()).split(s)).most_common(amount)\n",
    "    pd.DataFrame(temp, columns=['word', 'count']).plot(kind='bar', x='word', y='count')"
    "    \n",
    "    \n",
    "# takes in a dataframe, a grouping, and a column name and returns \n",
    "# the average of that column based on the group\n",
    "def average_by_group(df, group, column):\n",
    "    return df.groupBy(group).agg(avg(col(column)).alias(\"average\"))\n",
    "\n",
    "\n",
    "\n",
    "# All credit for this function goes to this site: https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
    "##takes a long ass time for descriptions ***********~40 mins*************\n",
    "# iterate through the column\n",
    "def wordCloud(df, column):\n",
    "    comment_words = ''\n",
    "    for val in df[column]: \n",
    "      \n",
    "        # typecaste each val to string \n",
    "        val = str(val) \n",
    "  \n",
    "        # split the value \n",
    "        tokens = val.split() \n",
    "      \n",
    "        # Converts each token into lowercase \n",
    "        for i in range(len(tokens)): \n",
    "            tokens[i] = tokens[i].lower() \n",
    "      \n",
    "        comment_words += \" \".join(tokens)+\" \"\n",
    "  \n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                    background_color ='black', \n",
    "                    stopwords = stopwords, \n",
    "                    min_font_size = 10).generate(comment_words)\n",
    "\n",
    "    # plot the WordCloud image                        \n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "  \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "def keyWords_filtered(df, column, amount):\n",
    "    pattern = '|'.join(['\\|', ';', '-', '\\(', '\\)', '\\[', '\\]', '&', ' ', '\\.', '\\,', '\\:', '\\/', '\\'', '\\!', '\\$', '’', '\\\\\\\\'])\n",
    "    some[column] = df[column].str.lower().str.replace(pattern, ' ').str.replace(r'\\b(\\w{1,3})\\b', '')\n",
    "    temp = Counter(\" \".join(some[column]).split()).most_common(20)\n",
    "    pd.DataFrame(temp, columns=['word', 'count']).plot(kind='bar', x='word', y='count')\n",
    "    \n",
    "\n",
    "def convert_weekday(num):\n",
    "    if num == 1:\n",
    "        return \"Monday\"\n",
    "    elif num ==2:\n",
    "        return \"Tuesday\"\n",
    "    elif num ==3:\n",
    "        return \"Wednesday\"\n",
    "    elif num ==4:\n",
    "        return \"Thursday\"\n",
    "    elif num ==5:\n",
    "        return \"Friday\"\n",
    "    elif num ==6:\n",
    "        return \"Saturday\"\n",
    "    elif num ==7:\n",
    "        return \"Sunday\"\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SparkSession.builder.appName(\"FinalProjectYoutube\").getOrCreate();\n",
    "root_dir = \"youtube-new/\";\n",
    "\n",
    "# Set up the schema for reading in the data sets into a dataframe\n",
    "customSchema = StructType([\n",
    "  StructField(\"video_id\", StringType(), True),\n",
    "  StructField(\"trending_date\", DateType(), True),\n",
    "  StructField(\"title\", StringType(), True),\n",
    "  StructField(\"channel_title\", StringType(), True),\n",
    "  StructField(\"category_id\", StringType(), True),\n",
    "  StructField(\"publish_time\", TimestampType(), True),\n",
    "  StructField(\"tags\", StringType(), True),\n",
    "  StructField(\"views\", IntegerType(), True),\n",
    "  StructField(\"likes\", IntegerType(), True),\n",
    "  StructField(\"dislikes\", IntegerType(), True),\n",
    "  StructField(\"comment_count\", IntegerType(), True),\n",
    "  StructField(\"thumbnail_link\", StringType(), True),\n",
    "  StructField(\"comments_disabled\", BooleanType(), True),\n",
    "  StructField(\"ratings_disabled\", BooleanType(), True),\n",
    "  StructField(\"video_error_or_removed\", BooleanType(), True),\n",
    "  StructField(\"description\", StringType(), True),\n",
    "  StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Associate csv files with respective countries\n",
    "data_files = [\n",
    "   (root_dir + \"CAvideos.csv\", \"Canada\"),\n",
    "   (root_dir + \"DEvideos.csv\", \"Germany\"),\n",
    "   (root_dir + \"FRvideos.csv\", \"France\"),\n",
    "   (root_dir + \"GBvideos.csv\", \"England\"),\n",
    "   (root_dir + \"INvideos.csv\", \"India\"),\n",
    "   (root_dir + \"JPvideos.csv\", \"Japan\"),\n",
    "   (root_dir + \"KRvideos.csv\", \"South Korea\"),\n",
    "   (root_dir + \"MXvideos.csv\", \"Mexico\"),\n",
    "   (root_dir + \"RUvideos.csv\", \"Russia\"),\n",
    "   (root_dir + \"USvideos.csv\", \"US\"),\n",
    "]\n",
    "\n",
    "us_data_file = [\n",
    "    (root_dir + \"USvideos.csv\", \"US\")\n",
    "]\n",
    "\n",
    "# Read in datasets\n",
    "youtube_data_df = Load_Datasets(sqlContext, data_files, customSchema)\n",
    "us_data_df = Load_Datasets(sqlContext, us_data_file, customSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When dropping the duplicate rows based on video_id we found that half of the dataset gets dropped.  So we now look at the \n",
    "# duplicate rows to find out more information about what is going on.  Looking at the duplicate rows you can see that the \n",
    "# same video can be trending for mulitple days and in different countries causing the same video to exist in multiple rows.\n",
    "# We decided the \"duplicates\" were not actually truely duplicate rows, the information provided by these multiple entries \n",
    "# is still useful.  We determined a truely duplicate row requires the same: \"video_id\", \"views\", \"likes\", \"dislikes\",\n",
    "# \"country\", and \"trending_date\" column values.\n",
    "pandasdf = pd.DataFrame(data=youtube_data_df.take(100000), columns=youtube_data_df.columns)\n",
    "\n",
    "pandas_df = pandasdf.loc[pandasdf['video_id'].duplicated()]\n",
    "\n",
    "# We realized that a number of the duplicates have a \"\\n\" for the video id, we decided to filter those rows out since\n",
    "# they contain no useful information.  We remove these rows later on when dropping NA values from the dataset.\n",
    "pandas_df = pandas_df[pandas_df['video_id'] != \"\\\\n\"]\n",
    "\n",
    "# The video id value was manually selected by viewing the resulting dataframe from the previous line. You can see\n",
    "# that this video was trending for multiple days and in mulitple countries, hence why it has mulitple rows in the \n",
    "# the dataset.\n",
    "pandasdf[pandasdf['video_id'] == \"n1WpP7iowLc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The previous section determined that video_id was not sufficient in determining a truly duplicate row.  The following\n",
    "# are a combination of the columns we think determine a truly duplicate row.  Meaning, if two rows have the same value\n",
    "# in all of the below columns, then those two rows are indeed duplicates. \n",
    "\n",
    "# We found that if we included views, likes,or dislikes as part of the criteria, then some duplicate rows still existed\n",
    "# because their view count differed only by roughly 10 views, but everyting else remained the same, which proves\n",
    "# problematic in future analysis\n",
    "compare_duplicate_cols = [\"video_id\", \"country\", \"trending_date\"]\n",
    "row_count_with_dup = youtube_data_df.count()\n",
    "\n",
    "# Drop duplicate rows, in the event where there is a duplicate row, we want to keep the one with the greatest number of\n",
    "# views, so we sort the rows by their views column in descending order.  This is useful because the dropDuplicates\n",
    "# function keeps the first instance of any pair of duplicate rows.\n",
    "youtube_data_df = youtube_data_df.orderBy(col(\"views\").desc()).dropDuplicates(compare_duplicate_cols)\n",
    "\n",
    "# View duplicate row count information\n",
    "num_duplicates = row_count_with_dup - youtube_data_df.count()\n",
    "print(\"Number of duplicates: \" + str(num_duplicates))\n",
    "print(\"Remaining number of rows: \" + str(youtube_data_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NA / Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are a list of the columns that we determined should not contain a null or NA value.  We could have\n",
    "# done this when specifying the schema when we were importing the data, but we felt it necessary to learn more information\n",
    "# about columns that contain nulls and NAs.  After playing around with the data we found that many of the values in the\n",
    "# description column were NA.  We decided that this was okay because some videos might not have a description, thus\n",
    "# this is why description is not included in the below list\n",
    "no_nan_cols = [\"video_id\", \"trending_date\", \"title\", \n",
    "        \"channel_title\", \"category_id\",  \n",
    "        \"tags\", \"views\", \"likes\", \"dislikes\", \"comment_count\", \n",
    "        \"thumbnail_link\", \"comments_disabled\", \"ratings_disabled\", \n",
    "        \"video_error_or_removed\", \"country\"]\n",
    "\n",
    "row_count_with_nans = youtube_data_df.count()\n",
    "\n",
    "# Drop nans from these columns\n",
    "youtube_data_df = youtube_data_df.na.drop(subset=no_nan_cols) \n",
    "\n",
    "# View duplicate row count information\n",
    "num_nans = row_count_with_nans - youtube_data_df.count()\n",
    "print(\"Number of null,nans, and na's: \" + str(num_nans))\n",
    "print(\"Remaining number of rows: \" + str(youtube_data_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We decided that it was advantageous to replace all of the nulls in the description column with empty strings, this\n",
    "# way we do not need to check for nulls later on when working with this column in the dataset\n",
    "youtube_data_df = youtube_data_df.fillna(\"\", subset=\"description\")\n",
    "\n",
    "# Check that the description column contains no nulls\n",
    "youtube_data_df.where(youtube_data_df.description.isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out #NAME? and #VALUE! from video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In later analysis we discovered that some rows contained either #NAME? or #VALUE! in their video_id column, which\n",
    "# we decided was unaccaptable due to questioning the validity of the data in that row.\n",
    "youtube_data_df = youtube_data_df.filter(youtube_data_df.video_id != \"#NAME?\")\n",
    "youtube_data_df = youtube_data_df.filter(youtube_data_df.video_id != \"#VALUE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping duplicates but keeping max views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following filters out the duplicate video entries, leaving only one entry, the one with the highest view count.\n",
    "# Having a separate dataframe setup this way proves convenient for some future analysis.\n",
    "\n",
    "# We decided to drop duplicate rows with the same video id and country, and keep the ones with the largest view count\n",
    "# The orderBy function call and the desc function call reorder the dataset rows in descending order based on the the\n",
    "# value in the views column.  We wanted this because dropDuplicates function keeps the first instance of any duplicate\n",
    "# pair, meaning in this case only the duplicate row with the highest view count will remain.\n",
    "youtube_nodup_df = youtube_data_df.orderBy(col(\"views\").desc()).dropDuplicates([\"video_id\", \"country\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Averages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Out of the trending videos, average number of views\n",
    "#change to not duplicate data instead of all_data\n",
    "average_by_group(youtube_nodup_df, \"country\", \"views\").toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_by_group(youtube_nodup_df, \"country\", \"likes\").toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_by_group(youtube_nodup_df, \"country\", \"dislikes\").toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "average_by_group(youtube_nodup_df, \"country\", \"comment_count\").toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [\"views\", 'likes', 'dislikes', 'comment_count']\n",
    "vector_col = \"corr_features\"\n",
    "\n",
    "inputdf = sqlContext.createDataFrame(youtube_nodup_df.select(*col).collect(), col)\n",
    "\n",
    "# convert to vector column first\n",
    "assembler = VectorAssembler(inputCols=inputdf.columns, outputCol=vector_col)\n",
    "df_vector = assembler.transform(inputdf).select(vector_col)\n",
    "\n",
    "# get correlation matrix\n",
    "matrix = Correlation.corr(df_vector, vector_col)\n",
    "print(str(matrix.head()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finding average title and description length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titlelen = nodup_data.withColumn('titleLength', length('title'))\n",
    "average_by_group(titlelen, 'country', 'titleLength').toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desclen = nodup_data.withColumn('descriptionLength', length('description'))\n",
    "average_by_group(desclen, 'country', 'descriptionLength').toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## possible word cloud\n",
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = us_data_df.toPandas()\n",
    "test = test[test['title'].notna()]\n",
    "test = test[test['description'].notna()]\n",
    "wordCloud(test, 'title', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is gunna take a very long time!!!!!!!!!!!!!!!!!!!!!!1\n",
    "#wordCloud(test, 'description', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword frequency count (us data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyWords(test, \"title\", \" \", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyWords(test, \"description\", \" \", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyWords_filtered(test, \"description\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Video Trending Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following counts the frequency a video_id appears in a country, which we are considering to be the trending \n",
    "# duration.  This of course does not account for videos that might re-trend in the same country, we attempted to\n",
    "# solve this problem but it proved too difficult for pyspark\n",
    "trend_video_duration_df = youtube_data_df.groupBy('video_id', 'country').count()\n",
    "trend_video_duration_df.show()\n",
    "average_by_group(trend_video_duration_df, \"country\", \"count\").toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is the average trending duration across all videos in all countries\n",
    "trend_video_duration_df.agg(avg(col(\"count\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Publish Day of Week Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section calculates what days of the week most trending videos are published on\n",
    "\n",
    "# This allows convert_weekday to be used on a pyspark column\n",
    "udf_convert_weekday = udf(convert_weekday, StringType())\n",
    "\n",
    "# Calculate the frequency of videos published for each day of the week\n",
    "publish_dow_freq = youtube_nodup_df.withColumn('Number Rep Day of Week', dayofweek(youtube_nodup_df.publish_time)).withColumn('Day of Week',udf_convert_weekday(dayofweek(youtube_nodup_df.publish_time))).groupBy(\"Day of Week\", 'Number Rep Day of Week').count().orderBy('Number Rep Day of Week').drop('Number Rep Day of Week')\n",
    "publish_dow_freq.show()\n",
    "\n",
    "# Plot the results\n",
    "ax = publish_dow_freq.toPandas().plot(kind='bar', figsize=[15,10], y = 'count', x = 'Day of Week', title='Number of Videos Published For Each Day of the Week')\n",
    "ax.set_xlabel(\"Day of Week\")\n",
    "ax.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most common tags\n",
    "keyWords(test, \"tags\", \"|\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word cloud of tags\n",
    "wordCloud(test, 'tags', '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average number of tags\n",
    "#first split the tags to get an array of tags per video (no duplicates)\n",
    "#then add a new column with the count of the number of tags\n",
    "#then find the average number of tags with the function\n",
    "data_tag_count = nodup_data.withColumn(\"tags\", split(\"tags\", \"\\|\")).withColumn(\"tag_count\", size(\"tags\"))\n",
    "average_by_group(data_tag_count, \"country\", \"tag_count\").toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trending Video Day of Week Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show total number of views by date\n",
    "views_by_date = youtube_data_df.groupBy(\"trending_date\").sum().orderBy(\"trending_date\").toPandas()\n",
    "pd.DataFrame(views_by_date, columns=['trending_date', 'sum(views)']).plot(kind='line', x='trending_date', y='sum(views)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to see the total number of views of trending videos spike a certain times of the year. We can see the total number increased in Feb of 2018 and decline in the middle of may. I hypothesize that this is because younger people mostly contribute to the status of trending videos and during these times, these people are usually busy with tests  or vacations instead of watching YouTube videos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keywords compare to trending date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#organize trending dates by week and concatenate the titles into one cell\n",
    "new_df = us_data_df.withColumn(\"week\",date_sub(next_day(col(\"trending_date\"),\"sunday\"),7)).groupBy(\"week\").agg(concat_ws(\", \", collect_list(us_data_df.title)).alias(\"title\")).orderBy(\"week\")\n",
    "new_df = new_df.toPandas().dropna()\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for all the titles, make them lowercased, get rid of special characters and words less then 3 characters long\n",
    "pattern = '|'.join(['\\|', ';', '-', '\\(', '\\)', '\\[', '\\]', '&', ' ', '\\.', '\\,', '\\:', '\\/', '\\'', '\\!', '\\$', '’', '\\\\\\\\', '\\'', '\\\"'])\n",
    "new_df[\"title\"] = new_df[\"title\"].str.lower().str.replace(pattern, ' ').str.replace(r'\\b(\\w{1,3})\\b', '')\n",
    "#get a new df with the frequency counts of each word\n",
    "freq = new_df[\"title\"].str.split().apply(pd.value_counts)\n",
    "#create a new df with the highest frequency, the word, and the date\n",
    "s = pd.DataFrame(freq.idxmax(axis=1), columns=[\"word\"])\n",
    "s[\"value\"] = pd.DataFrame(freq.max(axis=1), columns=['value'])\n",
    "s[\"week\"] = new_df[\"week\"]\n",
    "#plot\n",
    "s.sort_values('week').plot(kind='bar', x='word', y='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s #view table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what category had most trending videos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input the categories json as a dictionary\n",
    "categories={}\n",
    "data=pd.read_json(\"youtube-new/US_category_id.json\")\n",
    "for category in data[\"items\"]:\n",
    "    categories[category[\"id\"]]=category[\"snippet\"][\"title\"]#it Stores the category id with category name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories #display categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get to duplicates of us data\n",
    "nodup_us_data = us_data_df.orderBy(col(\"views\").desc()).dropDuplicates([\"video_id\", \"country\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_data = nodup_us_data.toPandas()\n",
    "#make the id a string because the dict entries are strings\n",
    "us_data[\"category_id\"]=us_data[\"category_id\"].astype(str)\n",
    "us_data[\"Category\"]=us_data[\"category_id\"].map(categories) #\"categories\" stores categories from JSON files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by category and plot \n",
    "us_data.groupby([\"Category\"]).count().plot(kind='bar', y=\"video_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what channel title has the most trending videos (in this time period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#want only one count of each video with the most number of views\n",
    "singleVideo_data = youtube_data_df.orderBy(col(\"views\").desc()).dropDuplicates([\"video_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of video each channel published\n",
    "top_channels = singleVideo_data.groupBy(\"channel_title\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#organize by channels with the most pubilshed videos within this time\n",
    "#take the top 20 and plot\n",
    "top_channels.orderBy(top_channels['count'].desc()).limit(20).toPandas().plot(kind='bar', x='channel_title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what channel title has the most total views (in this time period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#sum each channels total number of views within this time period\n",
    "top_views_channels = singleVideo_data.groupBy(\"channel_title\").sum().select(['channel_title', 'sum(views)'])\n",
    "#plot highest number of views per channel\n",
    "top_views_channels.orderBy(top_views_channels['sum(views)'].desc()).limit(20).toPandas().plot(kind='bar', x='channel_title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average time between publish time to trending time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want single video data but with the shortest time between publish and trending times\n",
    "leastTime_data = youtube_data_df.orderBy(col(\"views\")).dropDuplicates([\"video_id\"])\n",
    "date_differences_df = leastTime_data.withColumn(\"difference\", datediff(col(\"trending_date\"), col(\"publish_time\"))).select(\"difference\", \"publish_time\", \"trending_date\", \"country\", \"channel_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_by_group(date_differences_df, \"country\", \"difference\").toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## which channel after posting a video is the fastest to trending?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by channel title and take an average of the difference since trending date could be different in different countries\n",
    "#take the first 40 channels\n",
    "avg_day_trending = date_differences_df.groupBy(\"channel_title\").agg(avg('difference')).orderBy(\"avg(difference)\").na.drop().limit(40)\n",
    "avg_day_trending.toPandas()\n",
    "#data might be skewed for users with only one tredning videos vs others with many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section calculates what days of the week most trending videos are actually trending\n",
    "\n",
    "# This allows convert_weekday to be used on a pyspark column\n",
    "udf_convert_weekday = udf(convert_weekday, StringType())\n",
    "\n",
    "# Calculate the frequency of videos published for each day of the week\n",
    "publish_dow_freq = youtube_nodup_df.withColumn('Number Rep Day of Week', dayofweek(youtube_nodup_df.trending_date)).withColumn('Day of Week',udf_convert_weekday(dayofweek(youtube_nodup_df.trending_date))).groupBy(\"Day of Week\", 'Number Rep Day of Week').count().orderBy('Number Rep Day of Week').drop('Number Rep Day of Week')\n",
    "publish_dow_freq.show()\n",
    "\n",
    "# Plot the results\n",
    "ax = publish_dow_freq.toPandas().plot(kind='bar', figsize=[15,10], y = 'count', x = 'Day of Week', title='Number of Videos That Started Trending on a Particular Day of the Week')\n",
    "ax.set_xlabel(\"Day of Week\")\n",
    "ax.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
     "#what about us channels?\n",
     "#we want single video data but with the shortest time between publish and trending times\n",
     "leastTime_data = us_data_df.orderBy(col(\"views\")).dropDuplicates([\"video_id\"])\n",
     "#find the difference in publish and trending dates\n",
     "date_differences_df = leastTime_data.withColumn(\"difference\", datediff(col(\"trending_date\"), col(\"publish_time\"))).select(\"difference\", \"publish_time\", \"trending_date\", \"country\", \"channel_title\")\n",
     "#take the averages of differences and take top 40 channels\n",
     "avg_day_trending = date_differences_df.groupBy(\"channel_title\").agg(avg('difference')).orderBy(\"avg(difference)\").na.drop().limit(40)\n",
     "avg_day_trending.toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
