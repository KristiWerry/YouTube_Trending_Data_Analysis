{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "CS696 Big Data\n",
    "\n",
    "Professor Whitney\n",
    "\n",
    "Team:\n",
    "\n",
    "Kristi Werry - 823386935\n",
    "\n",
    "William Ritchie - 815829203"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import to_timestamp, max, length, dayofweek\n",
    "from pyspark.sql.functions import isnan, when, count, col, max\n",
    "from pyspark.sql.functions import lit, avg, format_number, corr, udf\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Datasets(sqlContext, files, schema):\n",
    "    data = sqlContext.createDataFrame([], schema=schema)\n",
    "    for file in files:\n",
    "        temp_data = sqlContext.read.csv(path=file[0], schema=schema, dateFormat=\"yy.dd.MM\", timestampFormat=\"yyyy-MM-dd\")\n",
    "        temp_data = temp_data.withColumn(\"country\", lit(file[1]))\n",
    "        data = data.union(temp_data)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def keyWords(df, column, amount):\n",
    "    temp = Counter(\" \".join(df[column].str.lower()).split()).most_common(amount)\n",
    "    pd.DataFrame(temp, columns=['word', 'count']).plot(kind='bar', x='word', y='count')\n",
    "\n",
    "    \n",
    "    \n",
    "# takes in a dataframe, a grouping, and a column name and returns \n",
    "# the average of that column based on the group\n",
    "def average_by_group(df, group, column):\n",
    "    return df.groupBy(group).agg(avg(col(column)).alias(\"average\"))\n",
    "\n",
    "\n",
    "\n",
    "# All credit for this function goes to this site: https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
    "##takes a long ass time for descriptions ***********~40 mins*************\n",
    "# iterate through the column\n",
    "def wordCloud(df, column):\n",
    "    comment_words = ''\n",
    "    for val in df[column]: \n",
    "      \n",
    "        # typecaste each val to string \n",
    "        val = str(val) \n",
    "  \n",
    "        # split the value \n",
    "        tokens = val.split() \n",
    "      \n",
    "        # Converts each token into lowercase \n",
    "        for i in range(len(tokens)): \n",
    "            tokens[i] = tokens[i].lower() \n",
    "      \n",
    "        comment_words += \" \".join(tokens)+\" \"\n",
    "  \n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                    background_color ='black', \n",
    "                    stopwords = stopwords, \n",
    "                    min_font_size = 10).generate(comment_words)\n",
    "\n",
    "    # plot the WordCloud image                        \n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "  \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "def keyWords_filtered(df, column, amount):\n",
    "    pattern = '|'.join(['\\|', ';', '-', '\\(', '\\)', '\\[', '\\]', '&', ' ', '\\.', '\\,', '\\:', '\\/', '\\'', '\\!', '\\$', 'â€™', '\\\\\\\\'])\n",
    "    some[column] = df[column].str.lower().str.replace(pattern, ' ').str.replace(r'\\b(\\w{1,3})\\b', '')\n",
    "    temp = Counter(\" \".join(some[column]).split()).most_common(20)\n",
    "    pd.DataFrame(temp, columns=['word', 'count']).plot(kind='bar', x='word', y='count')\n",
    "    \n",
    "\n",
    "def convert_weekday(num):\n",
    "    if num == 1:\n",
    "        return \"Monday\"\n",
    "    elif num ==2:\n",
    "        return \"Tuesday\"\n",
    "    elif num ==3:\n",
    "        return \"Wednesday\"\n",
    "    elif num ==4:\n",
    "        return \"Thursday\"\n",
    "    elif num ==5:\n",
    "        return \"Friday\"\n",
    "    elif num ==6:\n",
    "        return \"Saturday\"\n",
    "    elif num ==7:\n",
    "        return \"Sunday\"\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SparkSession.builder.appName(\"FinalProjectYoutube\").getOrCreate();\n",
    "root_dir = \"youtube-new/\";\n",
    "\n",
    "# Set up the schema for reading in the data sets into a dataframe\n",
    "customSchema = StructType([\n",
    "  StructField(\"video_id\", StringType(), True),\n",
    "  StructField(\"trending_date\", DateType(), True),\n",
    "  StructField(\"title\", StringType(), True),\n",
    "  StructField(\"channel_title\", StringType(), True),\n",
    "  StructField(\"category_id\", StringType(), True),\n",
    "  StructField(\"publish_time\", TimestampType(), True),\n",
    "  StructField(\"tags\", StringType(), True),\n",
    "  StructField(\"views\", IntegerType(), True),\n",
    "  StructField(\"likes\", IntegerType(), True),\n",
    "  StructField(\"dislikes\", IntegerType(), True),\n",
    "  StructField(\"comment_count\", IntegerType(), True),\n",
    "  StructField(\"thumbnail_link\", StringType(), True),\n",
    "  StructField(\"comments_disabled\", BooleanType(), True),\n",
    "  StructField(\"ratings_disabled\", BooleanType(), True),\n",
    "  StructField(\"video_error_or_removed\", BooleanType(), True),\n",
    "  StructField(\"description\", StringType(), True),\n",
    "  StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Associate csv files with respective countries\n",
    "data_files = [\n",
    "   (root_dir + \"CAvideos.csv\", \"Canada\"),\n",
    "   (root_dir + \"DEvideos.csv\", \"Germany\"),\n",
    "   (root_dir + \"FRvideos.csv\", \"France\"),\n",
    "   (root_dir + \"GBvideos.csv\", \"England\"),\n",
    "   (root_dir + \"INvideos.csv\", \"India\"),\n",
    "   (root_dir + \"JPvideos.csv\", \"Japan\"),\n",
    "   (root_dir + \"KRvideos.csv\", \"South Korea\"),\n",
    "   (root_dir + \"MXvideos.csv\", \"Mexico\"),\n",
    "   (root_dir + \"RUvideos.csv\", \"Russia\"),\n",
    "   (root_dir + \"USvideos.csv\", \"US\"),\n",
    "]\n",
    "\n",
    "# Read in datasets\n",
    "youtube_data_df = Load_Datasets(sqlContext, data_files, customSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When dropping the duplicate rows based on video_id we found that half of the dataset gets dropped.  So we now look at the \n",
    "# duplicate rows to find out more information about what is going on.  Looking at the duplicate rows you can see that the \n",
    "# same video can be trending for mulitple days and in different countries causing the same video to exist in multiple rows.\n",
    "# We decided the \"duplicates\" were not actually truely duplicate rows, the information provided by these multiple entries \n",
    "# is still useful.  We determined a truely duplicate row requires the same: \"video_id\", \"views\", \"likes\", \"dislikes\",\n",
    "# \"country\", and \"trending_date\" column values.\n",
    "pandasdf = pd.DataFrame(data=youtube_data_df.take(100000), columns=youtube_data_df.columns)\n",
    "\n",
    "pandas_df = pandasdf.loc[pandasdf['video_id'].duplicated()]\n",
    "\n",
    "# We realized that a number of the duplicates have a \"\\n\" for the video id, we decided to filter those rows out since\n",
    "# they contain no useful information.  We remove these rows later on when dropping NA values from the dataset.\n",
    "pandas_df = pandas_df[pandas_df['video_id'] != \"\\\\n\"]\n",
    "\n",
    "# The video id value was manually selected by viewing the resulting dataframe from the previous line. You can see\n",
    "# that this video was trending for multiple days and in mulitple countries, hence why it has mulitple rows in the \n",
    "# the dataset.\n",
    "pandasdf[pandasdf['video_id'] == \"n1WpP7iowLc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The previous section determined that video_id was not sufficient in determining a truly duplicate row.  The following\n",
    "# are a combination of the columns we think determine a truly duplicate row.  Meaning, if two rows have the same value\n",
    "# in all of the below columns, then those two rows are indeed duplicates. \n",
    "\n",
    "# We found that if we included views, likes,or dislikes as part of the criteria, then some duplicate rows still existed\n",
    "# because their view count differed only by roughly 10 views, but everyting else remained the same, which proves\n",
    "# problematic in future analysis\n",
    "compare_duplicate_cols = [\"video_id\", \"country\", \"trending_date\"]\n",
    "row_count_with_dup = youtube_data_df.count()\n",
    "\n",
    "# Drop duplicate rows, in the event where there is a duplicate row, we want to keep the one with the greatest number of\n",
    "# views, so we sort the rows by their views column in descending order.  This is useful because the dropDuplicates\n",
    "# function keeps the first instance of any pair of duplicate rows.\n",
    "youtube_data_df = youtube_data_df.orderBy(col(\"views\").desc()).dropDuplicates(compare_duplicate_cols)\n",
    "\n",
    "# View duplicate row count information\n",
    "num_duplicates = row_count_with_dup - youtube_data_df.count()\n",
    "print(\"Number of duplicates: \" + str(num_duplicates))\n",
    "print(\"Remaining number of rows: \" + str(youtube_data_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NA / Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are a list of the columns that we determined should not contain a null or NA value.  We could have\n",
    "# done this when specifying the schema when we were importing the data, but we felt it necessary to learn more information\n",
    "# about columns that contain nulls and NAs.  After playing around with the data we found that many of the values in the\n",
    "# description column were NA.  We decided that this was okay because some videos might not have a description, thus\n",
    "# this is why description is not included in the below list\n",
    "no_nan_cols = [\"video_id\", \"trending_date\", \"title\", \n",
    "        \"channel_title\", \"category_id\",  \n",
    "        \"tags\", \"views\", \"likes\", \"dislikes\", \"comment_count\", \n",
    "        \"thumbnail_link\", \"comments_disabled\", \"ratings_disabled\", \n",
    "        \"video_error_or_removed\", \"country\"]\n",
    "\n",
    "row_count_with_nans = youtube_data_df.count()\n",
    "\n",
    "# Drop nans from these columns\n",
    "youtube_data_df = youtube_data_df.na.drop(subset=no_nan_cols) \n",
    "\n",
    "# View duplicate row count information\n",
    "num_nans = row_count_with_nans - youtube_data_df.count()\n",
    "print(\"Number of null,nans, and na's: \" + str(num_nans))\n",
    "print(\"Remaining number of rows: \" + str(youtube_data_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We decided that it was advantageous to replace all of the nulls in the description column with empty strings, this\n",
    "# way we do not need to check for nulls later on when working with this column in the dataset\n",
    "youtube_data_df = youtube_data_df.fillna(\"\", subset=\"description\")\n",
    "\n",
    "# Check that the description column contains no nulls\n",
    "youtube_data_df.where(youtube_data_df.description.isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out #NAME? and #VALUE! from video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In later analysis we discovered that some rows contained either #NAME? or #VALUE! in their video_id column, which\n",
    "# we decided was unaccaptable due to questioning the validity of the data in that row.\n",
    "youtube_data_df = youtube_data_df.filter(youtube_data_df.video_id != \"#NAME?\")\n",
    "youtube_data_df = youtube_data_df.filter(youtube_data_df.video_id != \"#VALUE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping duplicates but keeping max views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following filters out the duplicate video entries, leaving only one entry, the one with the highest view count.\n",
    "# Having a separate dataframe setup this way proves convenient for some future analysis.\n",
    "\n",
    "# We decided to drop duplicate rows with the same video id and country, and keep the ones with the largest view count\n",
    "# The orderBy function call and the desc function call reorder the dataset rows in descending order based on the the\n",
    "# value in the views column.  We wanted this because dropDuplicates function keeps the first instance of any duplicate\n",
    "# pair, meaning in this case only the duplicate row with the highest view count will remain.\n",
    "youtube_nodup_df = youtube_data_df.orderBy(col(\"views\").desc()).dropDuplicates([\"video_id\", \"country\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Averages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Out of the trending videos, average number of views\n",
    "#change to not duplicate data instead of all_data\n",
    "average_by_group(youtube_nodup_df, \"country\", \"views\").toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_by_group(youtube_nodup_df, \"country\", \"likes\").toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_by_group(youtube_nodup_df, \"country\", \"dislikes\").toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "average_by_group(youtube_nodup_df, \"country\", \"comment_count\").toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [\"views\", 'likes', 'dislikes', 'comment_count']\n",
    "vector_col = \"corr_features\"\n",
    "\n",
    "inputdf = sqlContext.createDataFrame(youtube_nodup_df.select(*col).collect(), col)\n",
    "\n",
    "# convert to vector column first\n",
    "assembler = VectorAssembler(inputCols=inputdf.columns, outputCol=vector_col)\n",
    "df_vector = assembler.transform(inputdf).select(vector_col)\n",
    "\n",
    "# get correlation matrix\n",
    "matrix = Correlation.corr(df_vector, vector_col)\n",
    "print(str(matrix.head()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finding average title and description length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titlelen = nodup_data.withColumn('titleLength', length('title'))\n",
    "average_by_group(titlelen, 'country', 'titleLength').toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desclen = nodup_data.withColumn('descriptionLength', length('description'))\n",
    "average_by_group(desclen, 'country', 'descriptionLength').toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## possible word cloud\n",
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = us_data.toPandas()\n",
    "test = test[test['title'].notna()]\n",
    "test = test[test['description'].notna()]\n",
    "wordCloud(test, 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is gunna take a very long time!!!!!!!!!!!!!!!!!!!!!!1\n",
    "#wordCloud(test, 'description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword frequency count (us data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyWords(test, \"title\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyWords(test, \"description\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyWords_filtered(test, \"title\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyWords_filtered(test, \"description\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Video Trending Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following counts the frequency a video_id appears in a country, which we are considering to be the trending \n",
    "# duration.  This of course does not account for videos that might re-trend in the same country, we attempted to\n",
    "# solve this problem but it proved too difficult for pyspark\n",
    "trend_video_duration_df = youtube_data_df.groupBy('video_id', 'country').count()\n",
    "trend_video_duration_df.show()\n",
    "average_by_group(trend_video_duration_df, \"country\", \"count\").toPandas().plot(kind='bar', y='average', x='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is the average trending duration across all videos in all countries\n",
    "trend_video_duration_df.agg(avg(col(\"count\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Publish Day of Week Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section calculates what days of the week most trending videos are published on\n",
    "\n",
    "# This allows convert_weekday to be used on a pyspark column\n",
    "udf_convert_weekday = udf(convert_weekday, StringType())\n",
    "\n",
    "# Calculate the frequency of videos published for each day of the week\n",
    "publish_dow_freq = youtube_nodup_df.withColumn('Number Rep Day of Week', dayofweek(youtube_nodup_df.publish_time)).withColumn('Day of Week',udf_convert_weekday(dayofweek(youtube_nodup_df.publish_time))).groupBy(\"Day of Week\", 'Number Rep Day of Week').count().orderBy('Number Rep Day of Week').drop('Number Rep Day of Week')\n",
    "publish_dow_freq.show()\n",
    "\n",
    "# Plot the results\n",
    "ax = publish_dow_freq.toPandas().plot(kind='bar', figsize=[15,10], y = 'count', x = 'Day of Week', title='Number of Videos Published For Each Day of the Week')\n",
    "ax.set_xlabel(\"Day of Week\")\n",
    "ax.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trending Video Day of Week Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section calculates what days of the week most trending videos are actually trending\n",
    "\n",
    "# This allows convert_weekday to be used on a pyspark column\n",
    "udf_convert_weekday = udf(convert_weekday, StringType())\n",
    "\n",
    "# Calculate the frequency of videos published for each day of the week\n",
    "publish_dow_freq = youtube_nodup_df.withColumn('Number Rep Day of Week', dayofweek(youtube_nodup_df.trending_date)).withColumn('Day of Week',udf_convert_weekday(dayofweek(youtube_nodup_df.trending_date))).groupBy(\"Day of Week\", 'Number Rep Day of Week').count().orderBy('Number Rep Day of Week').drop('Number Rep Day of Week')\n",
    "publish_dow_freq.show()\n",
    "\n",
    "# Plot the results\n",
    "ax = publish_dow_freq.toPandas().plot(kind='bar', figsize=[15,10], y = 'count', x = 'Day of Week', title='Number of Videos That Started Trending on a Particular Day of the Week')\n",
    "ax.set_xlabel(\"Day of Week\")\n",
    "ax.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
